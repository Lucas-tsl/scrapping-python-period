# ğŸ•·ï¸ Web Scraping avec Python - Guide d'Apprentissage

Bienvenue dans ce repository dÃ©diÃ© Ã  l'apprentissage du web scraping avec Python ! Ce projet vous guidera Ã  travers les concepts fondamentaux et les techniques avancÃ©es pour extraire des donnÃ©es du web.

## ğŸ“‹ Table des MatiÃ¨res

- [Ã€ propos](#Ã -propos)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Technologies UtilisÃ©es](#technologies-utilisÃ©es)
- [Structure du Projet](#structure-du-projet)
- [Premiers Pas](#premiers-pas)
- [Exemples](#exemples)
- [Bonnes Pratiques](#bonnes-pratiques)
- [Ressources](#ressources)
- [Contribuer](#contribuer)
- [Licence](#licence)

## ğŸ¯ Ã€ propos

Ce projet est conÃ§u pour vous apprendre le web scraping de maniÃ¨re progressive, en commenÃ§ant par les bases et en explorant des techniques plus avancÃ©es. Vous apprendrez Ã  :

- Faire des requÃªtes HTTP avec `requests`
- Parser du HTML avec `BeautifulSoup4`
- Extraire et structurer des donnÃ©es
- GÃ©rer les erreurs et les exceptions
- Respecter les bonnes pratiques Ã©thiques du scraping

## âš™ï¸ PrÃ©requis

- Python 3.7 ou supÃ©rieur
- Connaissance de base en Python
- ComprÃ©hension basique du HTML
- Un Ã©diteur de code (VS Code recommandÃ©)

## ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone https://github.com/votre-username/scrapping-python-period.git
cd scrapping-python-period
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)

```bash
# macOS/Linux
python3 -m venv .venv
source .venv/bin/activate

# Windows
python -m venv .venv
.venv\Scripts\activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

Ou manuellement :

```bash
pip install requests beautifulsoup4
```

## ğŸ› ï¸ Technologies UtilisÃ©es

- **[Python 3](https://www.python.org/)** - Langage de programmation
- **[Requests](https://requests.readthedocs.io/)** - BibliothÃ¨que HTTP pour Python
- **[Beautiful Soup 4](https://www.crummy.com/software/BeautifulSoup/)** - BibliothÃ¨que de parsing HTML/XML
- **[lxml](https://lxml.de/)** (optionnel) - Parser XML/HTML plus rapide

## ğŸ“ Structure du Projet

```
scrapping-python-period/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ exemples/
â”‚   â”œâ”€â”€ 01_premier_scraper.py
â”‚   â”œâ”€â”€ 02_extraction_donnees.py
â”‚   â”œâ”€â”€ 03_gestion_erreurs.py
â”‚   â””â”€â”€ 04_scraper_avance.py
â”œâ”€â”€ projets/
â”‚   â”œâ”€â”€ scraper_citations/
â”‚   â”œâ”€â”€ scraper_actualites/
â”‚   â””â”€â”€ scraper_produits/
â””â”€â”€ data/
    â””â”€â”€ resultats/
```

## ğŸ“ Premiers Pas

### Exemple de base

Voici un exemple simple pour commencer :

```python
import requests
from bs4 import BeautifulSoup

# Faire une requÃªte HTTP
url = "https://example.com"
response = requests.get(url)

# Parser le HTML
soup = BeautifulSoup(response.content, 'html.parser')

# Extraire des donnÃ©es
titre = soup.find('h1').text
print(f"Titre de la page : {titre}")
```

### VÃ©rifier l'installation

```python
# test_installation.py
import requests
from bs4 import BeautifulSoup

print("âœ“ Requests version:", requests.__version__)
print("âœ“ BeautifulSoup installÃ© avec succÃ¨s!")
```

## ğŸ“š Exemples

### 1. Scraper Simple
```python
# Extraire tous les liens d'une page
links = soup.find_all('a')
for link in links:
    print(link.get('href'))
```

### 2. Extraire des DonnÃ©es StructurÃ©es
```python
# Extraire des articles de blog
articles = soup.find_all('article', class_='post')
for article in articles:
    titre = article.find('h2').text
    auteur = article.find('span', class_='author').text
    print(f"{titre} par {auteur}")
```

### 3. Sauvegarder en CSV
```python
import csv

# Sauvegarder les donnÃ©es
with open('data/resultats.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['Titre', 'Auteur', 'Date'])
    writer.writerow([titre, auteur, date])
```

## âœ… Bonnes Pratiques

### Respect des RÃ¨gles

1. **VÃ©rifier robots.txt** : Toujours consulter le fichier `robots.txt` du site
   ```
   https://example.com/robots.txt
   ```

2. **Respecter les dÃ©lais** : Ajouter des pauses entre les requÃªtes
   ```python
   import time
   time.sleep(1)  # Pause de 1 seconde
   ```

3. **User-Agent** : Identifier votre bot
   ```python
   headers = {
       'User-Agent': 'Mon Bot de Scraping 1.0 (contact@email.com)'
   }
   response = requests.get(url, headers=headers)
   ```

### Gestion des Erreurs

```python
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"Erreur : {e}")
```

### ConsidÃ©rations LÃ©gales et Ã‰thiques

- âš–ï¸ Respectez les conditions d'utilisation des sites
- ğŸ¤ Ne surchargez pas les serveurs
- ğŸ“œ VÃ©rifiez la lÃ©galitÃ© du scraping dans votre juridiction
- ğŸ”’ Ne scrapez pas de donnÃ©es personnelles sensibles
- ğŸ’¡ PrivilÃ©giez les APIs officielles quand elles existent

## ğŸ“– Ressources

### Documentation Officielle
- [Requests Documentation](https://requests.readthedocs.io/)
- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

### Tutoriels
- [Real Python - Web Scraping](https://realpython.com/python-web-scraping-practical-introduction/)
- [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)

### Sites pour Pratiquer
- [Quotes to Scrape](http://quotes.toscrape.com/) - Site d'entraÃ®nement
- [Books to Scrape](http://books.toscrape.com/) - Catalogue de livres fictifs
- [Scrape This Site](https://www.scrapethissite.com/) - Exercices de scraping

## ğŸ¤ Contribuer

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit vos changements (`git commit -m 'Ajout d'une fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ“§ Contact

Pour toute question ou suggestion :
- ğŸ“§ Email : votre.email@example.com
- ğŸ™ GitHub : [@votre-username](https://github.com/votre-username)

---

â­ Si ce projet vous aide, n'hÃ©sitez pas Ã  lui donner une Ã©toile !

**Happy Scraping! ğŸš€**
